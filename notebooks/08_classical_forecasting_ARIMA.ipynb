{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __AR(I)MA__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to ARIMA Models\n",
    "We'll investigate a variety of different forecasting models in upcoming sections, all of which are based on ARIMA.\n",
    "\n",
    "<b>ARIMA</b>, or <i>Autoregressive Integrated Moving Average</i> is a combination of 3 models:\n",
    "* <b>AR(p)</b> Autoregression - a regression model that utilizes the dependent relationship between a current observation and observations over a previous period\n",
    "* <b>I(d)</b> Integration - uses differencing of observations (subtracting an observation from an observation at the previous time step) in order to make the time series stationary\n",
    "* <b>MA(q)</b> Moving Average - a model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Autoregression _AR(p)___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_{t} = c + \\phi_{1}y_{t-1} + \\phi_{2}y_{t-2} + \\phi_{p}y_{t-p} + \\varepsilon_{t}$ <br/>\n",
    "<br/>\n",
    "$c$: constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Differencing _I(d)___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_{t}´ = y_t - y_{t-1}$ <br/>\n",
    "<br/>\n",
    "$y_{t}´´ = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Moving average model _MA(q)___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_{t} = c + \\varepsilon_{t} + \\theta_{1}\\varepsilon_{t-1} + \\theta_{2}\\varepsilon_{t-2} + \\theta_{q}\\varepsilon_{t-q}$ <br/>\n",
    "<br/>\n",
    "$c$: mean <br/>\n",
    "$\\epsilon$: error term\n",
    "\n",
    "__Example__\n",
    "\n",
    "$\\Delta lemonade_t = \\epsilon_t - 0.5\\epsilon_{t-1}$\n",
    "\n",
    "where $\\Delta lemonade_t$ is the change of lemonade consumption and where $\\epsilon_t$ represents a difference in temperature $\\Delta T$, both at time $t$. Now assume a temperature increase at time $t$ with $\\Delta T = positive$ while the temperature remains constant with $\\Delta T = 0$ at time $t+1$. Then we get:\n",
    "\n",
    "$\\Delta lemonade_{t+1} = - 0.5\\epsilon_{t}$\n",
    "\n",
    "Hence, we have a decrease in lemonade sales at time $t+1$.\n",
    "One way to interpret that example is that when the temperature increased at time $t$, people purchased more lemonade than they could consume at time $t$. Hence, they consumed some of that lemonade at time $t+1$ and therefore purchased less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __ARIMA(p,d,q)__\n",
    "\n",
    "ARIMA combines the three above components to make predictions.\n",
    "\n",
    "$y´_{t} = c + \\phi_{1}y´_{t-1} + ... + \\phi_{p}y´_{t-p} + \\theta_{1}\\varepsilon_{t-1} + ... + \\theta_{q}\\varepsilon_{t-q} + \\varepsilon_{t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARMA,ARMAResults,ARIMA,ARIMAResults\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "from pmdarima import auto_arima # on order to determine ARIMA orders\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../data/DailyTotalFemaleBirths.csv',index_col='Date',parse_dates=True)\n",
    "df1.index.freq = 'D'\n",
    "df1 = df1[:120]  # we only want the first four months\n",
    "\n",
    "df2 = pd.read_csv('../data/TradeInventories.csv',index_col='Date',parse_dates=True)\n",
    "df2.index.freq='MS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.info())\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.info())\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Augmented Dickey-Fuller Test__\n",
    "The Augmented Dickey-Fuller Test cheks for stationarity of a time series. Note that ARIMA can only be performed on stationary datasets. It might be possible that more than a one-time differencing is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a convenient fucntion to conduct the Augmented Dickey-Fuller Test\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(series,title=''):\n",
    "    \"\"\"\n",
    "    Pass in a time series and an optional title, returns an ADF report\n",
    "    \"\"\"\n",
    "    print(f'Augmented Dickey-Fuller Test: {title}')\n",
    "    result = adfuller(series.dropna(),autolag='AIC') # .dropna() since differencing produces NaNs\n",
    "    \n",
    "    labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "    out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "    for key,val in result[4].items():\n",
    "        out[f'critical value ({key})']=val\n",
    "        \n",
    "    print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Strong evidence against the null hypothesis\")\n",
    "        print(\"Reject the null hypothesis\")\n",
    "        print(\"Data has no unit root and is stationary\")\n",
    "    else:\n",
    "        print(\"Weak evidence against the null hypothesis\")\n",
    "        print(\"Fail to reject the null hypothesis\")\n",
    "        print(\"Data has a unit root and is non-stationary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __ARMA__\n",
    "\n",
    "ARMA is just a special case of ARIMA, namely when forecasting stationary data not requiring any differencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "df1['Births'].plot(figsize=(10,5), color='blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Dickey-Fuller Test to check for stationarity__\n",
    "\n",
    "$H_0$: Time series IS NOT stationary (if p-value > 0.05)\n",
    "\n",
    "$H_1$: Time series IS stationary (if p-value <= 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_test(df1['Births'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Determine the (p,q) ARMA orders using pmdarima.auto_arima__\n",
    "This tool provides best recommendations for p and q analogous to Grid Search in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Usage of Akaike Information Criterion (AIC)__\n",
    "\n",
    "AIC evaluates a collection of models and estimates the quality of each model __relative__ to the others. __Penalties__ are are provided for the number of parameters used in an effort to thwart overfitting.\n",
    "\n",
    "A good model is the one that has minimum AIC among all the other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_arima(df1['Births'],seasonal=False).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Test / train split__\n",
    "Apart from providing data, there is no room to tweak time series forecasts with any feature engineering. Hence, the risk of overfitting to the existing dataset is little, which is ẃhy we do not split the dataset into train / validation / test here, but only into train and test data.\n",
    "\n",
    "Rule of thumb: set the length of your test set equal to your intended forecast size. Here: 1 month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df1.iloc[:90]\n",
    "test = df1.iloc[90:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Fit ARMA(p,q) model__\n",
    "Also check out help(ARMA) to learn what incoming arguments are available/expected, and what's being returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARMA(train['Births'],order=(2,2))\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests we should use an ARMA(2,2) to fit our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Predicted values for single month__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=len(train)\n",
    "end=len(train)+len(test)-1\n",
    "predictions = results.predict(start=start, end=end).rename('ARMA(2,2) Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Plot predictions vs actuals__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Daily Total Female Births'\n",
    "ylabel='Births'\n",
    "xlabel='' # we don't really need a label here\n",
    "\n",
    "ax = test['Births'].plot(legend=True,figsize=(12,6),title=title)\n",
    "predictions.plot(legend=True)\n",
    "ax.autoscale(axis='x',tight=True)\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our starting dataset exhibited no trend or seasonal component, this prediction makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __ARIMA__\n",
    "\n",
    "Now we will use an non-stationary dataset, hence it requires differencing (_I_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker \n",
    "formatter = ticker.StrMethodFormatter('{x:,.0f}') #adding ticks to y-values\n",
    "\n",
    "title = 'Real Manufacturing and Trade Inventories'\n",
    "ylabel='Chained 2012 Dollars'\n",
    "xlabel='' # we don't really need a label here\n",
    "\n",
    "ax = df2['Inventories'].plot(figsize=(12,5),title=title, color='blue')\n",
    "ax.autoscale(axis='x',tight=True)\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "ax.yaxis.set_major_formatter(formatter);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Run seasonal_decompose to check for potential seasonality__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "result = seasonal_decompose(df2['Inventories'], model='additive')  # model='add' also works\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decomposition detects a seasonal component. However, its magnitude suggests only a minor effect. We hence treat our dataset as a non-seasonal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_arima(df2['Inventories'],seasonal=False).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests we should use an ARIMA(1,1,1) to fit our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Let's check all this manually (demonstration purpose only)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#let's difference once as suggested by auto_arima and see what the adf_test tells us\n",
    "from statsmodels.tsa.statespace.tools import diff\n",
    "df2['d1'] = diff(df2['Inventories'],k_diff=1)\n",
    "\n",
    "adf_test(df2['d1'],'Real Manufacturing and Trade Inventories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that we reached stationarity after the first difference as expected from auto_arima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Run the ACF and PACF plots__\n",
    "A pacf plot can reveal recommended AR(p) orders, and an acf plot can do the same for MA(q) orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Autocorrelation: Real Manufacturing and Trade Inventories'\n",
    "lags = 40\n",
    "plot_acf(df2['Inventories'],title=title,lags=lags);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Partial Autocorrelation: Real Manufacturing and Trade Inventories'\n",
    "lags = 40\n",
    "plot_pacf(df2['Inventories'],title=title,lags=lags);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the AR component should be more important than MA. From the <a href='https://people.duke.edu/~rnau/411arim3.htm'>Duke University Statistical Forecasting site</a>:<br>\n",
    "> <em>If the PACF displays a sharp cutoff while the ACF decays more slowly (i.e., has significant spikes at higher lags), we    say that the stationarized series displays an \"AR signature,\" meaning that the autocorrelation pattern can be explained more    easily by adding AR terms than by adding MA terms.</em><br>\n",
    "\n",
    "Let's take a look at <tt>pmdarima.auto_arima</tt> done stepwise to see if having $p$ and $q$ terms the same still makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_fit = auto_arima(df2['Inventories'], start_p=0, start_q=0,\n",
    "                          max_p=2, max_q=2, m=12,\n",
    "                          seasonal=False,\n",
    "                          d=None, trace=True,\n",
    "                          error_action='ignore',   # we don't want to know if an order does not work\n",
    "                          suppress_warnings=True,  # we don't want convergence warnings\n",
    "                          stepwise=True)           # set to stepwise\n",
    "\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our manual checkup on the p,d,q parameters confirmed our outcome of the initial auto_arima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __train / test split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set one year for testing\n",
    "train = df2.iloc[:252]\n",
    "test = df2.iloc[252:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Fit on ARIMA(1,1,1) model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(train['Inventories'],order=(1,1,1))\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain predicted values\n",
    "start=len(train)\n",
    "end=len(train)+len(test)-1\n",
    "predictions = results.predict(start=start, end=end, dynamic=False, typ='levels').rename('ARIMA(1,1,1) Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing dynamic=False means that forecasts at each point are generated using the full history up to that point (all lagged values).\n",
    "\n",
    "Passing typ='levels' predicts the levels of the original endogenous variables. If we'd used the default typ='linear' we would have seen linear predictions in terms of the differenced endogenous variables.\n",
    "\n",
    "More information on these arguments: https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima_model.ARIMAResults.predict.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare predictions to expected values\n",
    "for i in range(len(predictions)):\n",
    "    print(f\"predicted={round(predictions[i], 3)}, expected={test['Inventories'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot predictions vs actuals\n",
    "title = 'Real Manufacturing and Trade Inventories'\n",
    "ylabel='Chained 2012 Dollars'\n",
    "#xlabel redundant\n",
    "\n",
    "ax = test['Inventories'].plot(legend=True,figsize=(12,6),title=title)\n",
    "predictions.plot(legend=True)\n",
    "ax.autoscale(axis='x',tight=True)\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "ax.yaxis.set_major_formatter(formatter);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Model evaluation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "error = mean_squared_error(test['Inventories'], predictions)\n",
    "print(f'ARIMA(1,1,1) MSE Error: {error:11.10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "error = rmse(test['Inventories'], predictions)\n",
    "print(f'ARIMA(1,1,1) RMSE Error: {error:11.10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_error = error / predictions.mean()\n",
    "relative_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape = (sum(abs((test['Inventories'] - predictions)\\\n",
    "                /test['Inventories'])))*(100/len(test['Inventories']))\n",
    "mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remember MAPE represents a percentage value!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Apply model to complete dataset and forecast into future!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(df2['Inventories'],order=(1,1,1))\n",
    "results = model.fit()\n",
    "#forecast of 11 time steps\n",
    "forecast = results.predict(len(df2),len(df2)+11,typ='levels').rename('ARIMA(1,1,1) Forecast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions against known values\n",
    "title = 'Real Manufacturing and Trade Inventories'\n",
    "ylabel='Chained 2012 Dollars'\n",
    "xlabel='' # we don't really need a label here\n",
    "\n",
    "ax = df2['Inventories'].plot(legend=True,figsize=(12,6),title=title)\n",
    "forecast.plot(legend=True)\n",
    "ax.autoscale(axis='x',tight=True)\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "ax.yaxis.set_major_formatter(formatter);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsr_teaching",
   "language": "python",
   "name": "dsr_teaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
