{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Training an xgboost model__\n",
    "\n",
    "xgboost does not belong to classical time series models, however it is used frequently in the data science community for time series forecasts. The model uses base learners which are commonly decision trees. The training is based on gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Data preparation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/train_time_features.pkl'\n",
    "df = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t CO2-e / MWh    False\n",
       "year             False\n",
       "minute_sin       False\n",
       "minute_cos       False\n",
       "hour_sin         False\n",
       "hour_cos         False\n",
       "weekday_sin      False\n",
       "month_sin        False\n",
       "month_cos        False\n",
       "lag1              True\n",
       "lag2              True\n",
       "lag3              True\n",
       "lag4              True\n",
       "lag5              True\n",
       "lag6              True\n",
       "lag7              True\n",
       "lag8              True\n",
       "lag9              True\n",
       "lag10             True\n",
       "lag11             True\n",
       "lag12             True\n",
       "horizon0          True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our lagging operation caused the introduction of NaN values into our dataset which need to be removed before the xgboost\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t CO2-e / MWh    False\n",
       "year             False\n",
       "minute_sin       False\n",
       "minute_cos       False\n",
       "hour_sin         False\n",
       "hour_cos         False\n",
       "weekday_sin      False\n",
       "month_sin        False\n",
       "month_cos        False\n",
       "lag1             False\n",
       "lag2             False\n",
       "lag3             False\n",
       "lag4             False\n",
       "lag5             False\n",
       "lag6             False\n",
       "lag7             False\n",
       "lag8             False\n",
       "lag9             False\n",
       "lag10            False\n",
       "lag11            False\n",
       "lag12            False\n",
       "horizon0         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __train / validation split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_ts(df, relative_train, maximal_lag, horizon):\n",
    "    '''\n",
    "    Time series (ts) split function creates a train/test set under consideration of potential overlap between the two due to lag processing\n",
    "    X_train, y_train, X_test, y_test = ...\n",
    "    df=must contain target column as \"target\"; all other columns must be used as features\n",
    "    percentage_train=how much of the total dataset shall be used for training; must be added between 0 - 1\n",
    "    maximal_lag=out of all lag feature engineering, enter the maximal lag number\n",
    "    '''\n",
    "    k = int(df.shape[0] * relative_train)\n",
    "    data_train = df.iloc[:k,:]\n",
    "    #to avoid overlapping of train and test data, a gap of the maximal lag - 1 must be included between the two sets\n",
    "    data_test = df.iloc[k+maximal_lag:,:]\n",
    "    \n",
    "    assert data_train.index.max() < data_test.index.min()\n",
    "    \n",
    "    #returns in the sequence X_train, y_train, X_test, y_test\n",
    "    return (data_train.drop(columns=[f\"horizon{horizon}\",\"t CO2-e / MWh\"], axis=1), data_train[f\"horizon{horizon}\"],\n",
    "            data_test.drop(columns=[f\"horizon{horizon}\",\"t CO2-e / MWh\"], axis=1), data_test[f\"horizon{horizon}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Model training__\n",
    "\n",
    "Initially, we will do the model training without the lag features together. In an exerice, you will do it yourself with the entire feature set, i.e. including the lag features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['t CO2-e / MWh', 'year', 'minute_sin', 'minute_cos', 'hour_sin',\n",
      "       'hour_cos', 'weekday_sin', 'month_sin', 'month_cos', 'lag1', 'lag2',\n",
      "       'lag3', 'lag4', 'lag5', 'lag6', 'lag7', 'lag8', 'lag9', 'lag10',\n",
      "       'lag11', 'lag12', 'horizon0'],\n",
      "      dtype='object')\n",
      "2016-08-18 15:20:00\n",
      "2016-08-18 16:25:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/venvs/smc/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/user/venvs/smc/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:54:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=5, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=3, nthread=None, num_estimators=100, objective='reg:linear',\n",
       "             random_state=0, reg_alpha=0.05, reg_lambda=0, scale_pos_weight=1,\n",
       "             seed=None, silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_validation, y_validation = train_test_ts(\n",
    "    df=df.drop(columns=['lag1', 'lag2', 'lag3', 'lag4', 'lag5', 'lag6', 'lag7', 'lag8', 'lag9', 'lag10', 'lag11', 'lag12']),\n",
    "    relative_train=0.8,\n",
    "    maximal_lag=12,\n",
    "    horizon=0)\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "print(X_train.index.max())\n",
    "print(X_validation.index.min())\n",
    "\n",
    "assert X_train.index.max() < X_validation.index.min()\n",
    "\n",
    "model = xgb.XGBRegressor(max_depth=5,\n",
    "                         learning_rate=0.1,\n",
    "                         num_estimators=100,\n",
    "                         n_jobs=3,\n",
    "                         reg_alpha=0.05,\n",
    "                         reg_lambda=0,\n",
    "                        )\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(model, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    train_mae = (sum(abs(y_train - model.predict(X_train)))/len(y_train))\n",
    "    train_mape = (sum(abs((y_train - model.predict(X_train))/y_train)))*(100/len(y_train))\n",
    "    train_smape = sum(abs(y_train - model.predict(X_train)))/sum(y_train + model.predict(X_train))\n",
    "\n",
    "    test_mae = (sum(abs(y_test - model.predict(X_test)))/len(y_test))\n",
    "    test_mape = (sum(abs((y_test - model.predict(X_test))/y_test)))*(100/len(y_test))\n",
    "    test_smape = sum(abs(y_test - model.predict(X_test)))/sum(y_test + model.predict(X_test))\n",
    "\n",
    "    print(f'train_MAE: {train_mae}')\n",
    "    print(f'test_MAE: {test_mae}')\n",
    "    \n",
    "    print(f'train_MAPE: {train_mape}')\n",
    "    print(f'test_MAPE: {test_mape}')\n",
    "    \n",
    "    print(f'train_SMAPE: {train_smape}')\n",
    "    print(f'test_SMAPE: {test_smape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now we have successfully trained the model. However, we have not evaluated the model yet. Let's do that with our last notebook in mind.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Exercise 1:__\n",
    "\n",
    "Write a function which takes our train data (X_train, y_train), our validation data (X_test, y_test), and our trained model as input and which returns the MAE, MAPE, and SMAPE of the train and test data. Use your function to asses the errors of the train and of the validation set. What is it that the MAPE is showing and why? How do you interpret the outcomes of the train and validation errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Your solution 1:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    #your code here\n",
    "    \n",
    "    print(f'train_MAE: {train_mae}')\n",
    "    print(f'test_MAE: {test_mae}')\n",
    "    print(f'train_SMAPE: {train_SMAPE}')\n",
    "    print(f'test_SMAPE: {test_SMAPE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_MAE: 0.24546015365607068\n",
      "test_MAE: 0.2581517514093108\n",
      "train_MAPE: inf\n",
      "test_MAPE: inf\n",
      "train_SMAPE: 0.1742231695142011\n",
      "test_SMAPE: 0.18861056659382508\n"
     ]
    }
   ],
   "source": [
    "errors(model, X_train, y_train, X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Exercise 2:__\n",
    "\n",
    "Illustrate a comparison of the validation set (y_validation) and the forecasted values. Illustrate a period of i) 48 h and of ii) 4 h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Your solution 2:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Exercise 3:__\n",
    "\n",
    "Perform the xgboost training again using the entire dataframe including the lag features. Save the resulting model. Check the error metrics and visualise the results as above. What do you see? How do you interpret it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Your solution 3:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Exercise 4:__\n",
    "\n",
    "Take our test set and perform all data processing (cleaning, feature engineering) as we did with our training set. Use the saved model to make predicitons on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Your solution 4:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Feature importances__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsr_teaching",
   "language": "python",
   "name": "dsr_teaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
