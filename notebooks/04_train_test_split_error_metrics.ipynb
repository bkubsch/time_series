{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Preparation of train / validation split and training__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Splitting time series into train / test data__\n",
    "\n",
    "First of all, note that we have split off our test set already at the beginning. Test and validation splits are often confused.\n",
    "\n",
    "Have a closer look onto the table below. If we want to split our dataset into a train / test split, we have to think through a few points. Hypothetically, imagine we make a clean cut at any Timestamp > 2009-07-01 05:00:00. Then, almost all values of lag1 in the train set are present in lag12 of the test set. The same goes for all remaining lag columns in a lesser extent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/leaking_example.png\"> <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "#train / validation split can be done with sklearn.model_selection.train_test_split\n",
    "#the train_test_split can be done in few simple lines; here, 80 % of the dataset is assigned to become the training set\n",
    "\n",
    "k = int(df.shape[0] * 0.8)\n",
    "\n",
    "data_train = df.iloc[:k,:]\n",
    "data_validation = df.iloc[k:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Exercise 1__\n",
    "\n",
    "Consider what was mentioned above about train / test leaking and write a function which takes our dataframe df as an input and returns our dataset split into X_train, y_train, X_test, y_test considering for the avoidance of leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_ts(df, relative_train, maximal_lag, horizon):\n",
    "    \n",
    "    #your code here\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_ts(df, relative_train, maximal_lag, horizon):\n",
    "    '''\n",
    "    Time series (ts) split function creates a train/test set under consideration of potential overlap between the two due to lag processing\n",
    "    X_train, y_train, X_test, y_test = ...\n",
    "    df=must contain target column as \"target\"; all other columns must be used as features\n",
    "    percentage_train=how much of the total dataset shall be used for training; must be added between 0 - 1\n",
    "    maximal_lag=out of all lag feature engineering, enter the maximal lag number\n",
    "    '''\n",
    "    k = int(df.shape[0] * relative_train)\n",
    "    data_train = df.iloc[:k,:]\n",
    "    #to avoid overlapping of train and test data, a gap of the maximal lag - 1 must be included between the two sets\n",
    "    data_test = df.iloc[k+maximal_lag:,:]\n",
    "    \n",
    "    assert data_train.index.max() < data_test.index.min()\n",
    "    \n",
    "    #returns in the sequence X_train, y_train, X_test, y_test\n",
    "    return (data_train.drop(columns=[f\"horizon{horizon}\",\"t CO2-e / MWh\"], axis=1), data_train[f\"horizon{horizon}\"],\n",
    "            data_test.drop(columns=[f\"horizon{horizon}\",\"t CO2-e / MWh\"], axis=1), data_test[f\"horizon{horizon}\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/train_time_features.pkl'\n",
    "df = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.index.min())\n",
    "print(df.index.max())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_validation, y_validation = train_validation_ts(df, 0.8, 12, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Error metrics for time series data__\n",
    "\n",
    "Any data science project relies heavily on the metric - time series is no different.  \n",
    "\n",
    "See [Another look at measures of forecast accuracy - Hyndman](https://robjhyndman.com/publications/another-look-at-measures-of-forecast-accuracy/) for a detailed look at the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Mean Absolute Error (MAE)__\n",
    "\n",
    "Has the advantage of being in the same units as the data, eaisly interpretable\n",
    "\n",
    "Disadvantage of not allowing comparison between different time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MAE = \\frac{\\sum\\limits_{t=1}^n | F_t - A_t |}{n} $\n",
    "\n",
    "$F_t$: forecast value <br/>\n",
    "$A_t$: actual value <br/>\n",
    "$n$: sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Mean Absolute Percentage Error (MAPE)__\n",
    "\n",
    "Allows comparison between different time series.\n",
    "\n",
    "Disadvantage of being undefined at y=0, unsymmetric around 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MAPE = \\frac{100 \\%}{n}\\sum\\limits_{t=1}^n \\frac{A_t - F_t}{A_t}$\n",
    "\n",
    "$F_t$: forecast value <br/>\n",
    "$A_t$: actual value <br/>\n",
    "$n$: sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Symmetric Mean Absolute Percentage Error (SMAPE)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$SMAPE = \\frac{\\sum\\limits_{t=1}^n | F_t - A_t |}{\\sum\\limits_{t=1}^n (A_t + F_t)}$\n",
    "\n",
    "$F_t$: forecast value <br/>\n",
    "$A_t$: actual value <br/>\n",
    "$n$: sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Mean Absolute Scaled Error (MASE)__\n",
    "\n",
    "Scaling by the error of another (naive) forecast\n",
    "\n",
    "$> 1$ if the forecast is better than the naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MASE = \\frac{\\frac{1}{J}\\sum\\limits_{j} | \\epsilon_j | }{\\frac{1}{T-1} \\sum\\limits_{t=2}^T | Y_t - Y_{t-1} | }$\n",
    "<br/>\n",
    "<br/>\n",
    "$e_j$: forecast error of naive forecast for given period <br/>\n",
    "$J$: number of forecasts <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
